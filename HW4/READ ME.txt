This project consists of 2 sets of code
1) Sequence to Sequence using encoder/decoder with a GRU with no attention
2) Sequence to Sequence using encoder/decoder with a GRU using attention

Dataset:
The dataset is built into the code. If you wish to change the dataset to different languages,
make sure to keep it in pairs like the other dataset is and there should not have to be any
other changes.

Running The Code:
Make sure you have created the file structure shown below as if the folders do not exist,
it could create issues. Settings are at the top of each of the programs so you can easily
mess with the settings as you see fit.

HW4/
├─ graphs/
│  ├─ seq_to_seq/
│  ├─ seq_to_seq_attention/
├─ model/
│  ├─ with_attention/
│  ├─ without_attention/
├─ sequence_to_sequence_attention.py
├─ sequence_to_sequence.py


Results:
When you run any of the code, training loss and other validation metrics will be recorded. 
Graphs will be generated and stored in the 'graphs' folder along with a .txt file that 
has the values at each epoch. In both codes, the variable 'examplesToPrint' controls how 
many epochs it will wait before printing another line. If you are running a high number of epochs, 
setting this value can help stop the .txt from being too long. The model will be saved to 
the 'model' folder after the codes have finished.
